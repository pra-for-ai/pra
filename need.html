<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex">
    <link rel="icon" href="Images/favicon.png" type="image/x-icon">
    <title>PRA Framework: The Need for PRA in AI</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div class="header-content">
            <div class="header-title">
                <h1><a href="index.html" style="color: #F5F5F5; text-decoration: none;">PRA Framework</a></h1>
                <div class="subtitle">The Need for PRA in AI</div>
            </div>
            <nav>
                <a href="index.html">OVERVIEW</a>
                <a href="need.html">NEED</a>
                <a href="taxonomy.html">TAXONOMY</a>
                <a href="workbook.html">WORKBOOK</a>
            </nav>
        </div>
    </header>
    <div class="header-bar">‎</div>
    <main>
        <div class="content">
            <h2 style="width: 100%; font-weight: normal; color: #333333; margin-top: 0cm;">Characteristics of Increasingly General-Purpose AI Risk</h2>

            <p>Due to the unprecedented leverage that advanced AI affords, assessment of advanced AI must contend with relatively unique factors that differentiate it from many other types of risk assessments:</p>

            <ol>
                <li><strong>Global Impact:</strong> Given their widespread proliferation and adoption, and given the unprecedented leverage that such systems provide, AI systems have the potential for widespread, even global, effects.</li>
                <li><strong>Unprecedented Capabilities:</strong> AI systems are developing capabilities that are neither explicitly designed nor explicitly trained for, and which often have no historical precedent, making it difficult to assess risks based on past experiences.</li>
                <li><strong>Dual Use Threats:</strong> Malicious actors may attempt to exploit or manipulate AI models, or simply give them purposely harmful goals.</li>
                <li><strong>Vulnerability to Systemic Failures:</strong> Many AI safety measures rely on centralized controls or specific constraints. If these are compromised—for instance, through unauthorized access to model weights or unrestricted internet access—the consequences could be severe and far-reaching.</li>
                <li><strong>Rapid Evolution:</strong> The field of AI is advancing at an unprecedented pace, requiring risk assessment methods that can keep up with and anticipate new developments. AI agents can show unexpected capability jumps, potentially replicate and improve themselves at rapid speeds.</li>
                <li><strong>Loss of Control:</strong> The lack of warnings of risk, the complexity, and the speed at which initiating events of unintended side effects develop will often exceed the capacities of humans to stop those chains once they start.</li>
                <li><strong>Complex Interactions:</strong> AI systems can interact with their environment and other systems in ways that are difficult to predict, leading to emergent behaviors and risks.</li>
            </ol>

            <p>As AI models advance in size and capability, they demonstrate an increasingly general breadth of competences, within which they enable superhuman speed and abilities. Empirical evidence sets a lower bound on the capabilities and dangers that AI models can pose: deepfake creation, specific vectors of cyber offense uplift, and hallucination are of course examples of readily reproducible risks. Benchmarks and traditional evals are imperfect tools to measure these, but they get much of the way to what is needed to do so.</p>

            <p>But the risks posed by today's and tomorrow's models enter territory for which no precedent yet exists. Large problems lurk beyond the horizon of what can be tested in a few weeks. AI risk assessment must therefore draw more heavily on analytical models, theory, and expert testimony than most popular methods.</p>

            <h2>Need for AI Probabilistic Risk Assessment</h2>

            <p>These unique characteristics of AI systems create a pressing need for risk assessment methodologies. Key factors driving this need include:</p>
            <ul>
                <li><strong>Rapid Risk Development:</strong> The rapid pace of development towards general purpose AI systems that can bring with them dangerous hazards necessitates an assessment procedure to help ensure the safety of these systems.</li>
                <li><strong>Limitations of Current Methods:</strong> Existing approaches may not fully capture the complexity and scale of AI risks.</li>
                <li><strong>Proactive Assessment:</strong> There's a need for methods that can identify and evaluate potential risks before they manifest.</li>
                <li><strong>Regulatory Demand:</strong> Regulators need a broader and deeper mix of options for evaluating and setting thresholds for AI systems.</li>
            </ul>
            <p>The application of well-established risk management, system safety engineering, and probabilistic risk assessment methodologies to general-purpose AI systems has been conspicuously absent, despite their proven effectiveness in other fields. This makes the urgency of developing a structured risk assessment method for AI urgent and crucial.</p>

            <h2>Comparison of Risk Assessment Methods</h2>

            <p>Current AI risk evaluation methods vary in their approach and effectiveness. In particular, they vary across the following criteria for comprehensive threat surface analysis:</p>
            <ul>
                <li>A method can be more or less fine-grained. A fine-grained method can produce many different outputs, whereas a coarse-grained method might only produce a few.</li>
                <li>A method can be a better or worse proxy for safety.</li>
                <li>A method can cover more or less of the AI risk "threat surface," e.g., by number of threat models.</li>
                <li>A method can be more or less robust to mitigation failure. A robust method assumes mitigation failure.</li>
                <li>A method can guide the assessor over the threat surface of AI risk — or, it can lack such guidance.</li>
                <li>A method can be more or less informed by a specific system's characteristics — for example, specific capabilities or features.</li>
                <li>A method can support prospective analysis of a system's risk — that is, it can identify leading indicators of risk, rather than retrospective evaluation once a system is already developed or deployed.</li>
                <li>A method can be more or less "objective." An objective method requires little or no subjective input — more example, a benchmark is an objective method.</li>
                <li>A method can consider harm severity in addition to likelihood, or only consider likelihood.</li>
            </ul>
            <p>The image below compares current evaluation methods across these criteria. Scores were assigned based on expert analysis of each method’s capabilities and limitations. High (H) indicates strong performance or comprehensive coverage of the criterion, Medium (M) indicates partial fulfillment, and Low (L) indicates minimal or no attention to the criterion.</p>
            
            <figure>
                <img src="Images/heatmap.png" alt="Comparison of Risk Assessment Methods" class="flowchart">
                <figcaption>Figure 1: Comparison of Risk Assessment Methods.</figcaption>
            </figure>

            <p>We find that no single method excels across all criteria, highlighting the need for a multi-faceted approach to AI risk assessment. In particular, many current methods struggle with threat surface coverage and robustness to mitigation failure. However, probabilistic risk assessment shows the most balanced performance across dimensions.</p>

            <h2>Benefits of the PRA for AI Framework</h2>
            <p>The Probabilistic Risk Assessment (PRA) AI framework offers several key benefits that address the unique challenges of AI risk assessment:</p>
            <ul>
                <li><strong>Probabilistic Risk Assessment:</strong> The framework provides a structured approach to evaluate and quantify potential societal risks from AI systems. It can accommodate variable-resolution and variable-certainty analysis of risk pathways so as to allow for a nuanced understanding of complex risk scenarios.</li>
                <li><strong>Comprehensive System Aspect Coverage:</strong> The framework uses a Taxonomy of Aspect-Oriented AI Hazards that covers AI Capabilities, Knowledge Domains, operational Affordances, and sociotechnical Impact Domains. It captures first and second order risks, considering effects on individuals, society, and the biosphere.</li>
                <li><strong>Top-down View of AI Hazards:</strong> The framework offers a top-down perspective on the hazards from general purpose AI systems. This includes a structured approach to examining both competence-based and incompetence-based hazards across various aspects of AI systems.</li>
                <li><strong>Consideration of Direct and Systemic Risks:</strong> The framework allows for the assessment of both immediate, direct harms and more subtle, long-term systemic risks.</li>
                <li><strong>Explicit Assumptions and Likelihood Documentation:</strong> The framework prompts assessors to make their assumptions and likelihood levels estimates explicit and granular. This transparency enhances the reliability and reproducibility of the assessment process.</li>
            </ul>

            <h2>Weaknesses of the PRA for AI Framework</h2>
            <p>There are some weaknesses in the PRA for AI framework: </p>
            <ul>
                <li><strong>Subjectivity in probability estimates:</strong> Despite structured approaches, assessors may struggle to accurately estimate probabilities for novel or complex AI risks, potentially leading to inconsistent assessments.</li>
                <li><strong>Rapid AI advancement challenges:</strong> The fast-paced evolution of AI capabilities may outpace the assessment process, making some evaluations quickly outdated.</li>
                <li><strong>Unknown unknowns:</strong> PRA may not adequately capture unforeseen risks or emergent behaviors in advanced AI systems.</li>
                <li><strong>Potential for anchoring bias:</strong> Predefined categories and examples might inadvertently anchor assessors' thinking, limiting consideration of unconventional scenarios.</li>
                <li><strong>Resource intensity:</strong> Thorough PRAs require significant time and expertise, which may be challenging for smaller organizations or rapid development cycles.</li>
                <li><strong>Limited empirical validation:</strong> Due to the novelty of advanced AI systems, there's a lack of historical data to validate risk assessments, potentially reducing their reliability.</li>
            </ul>

            <!-- <p>For a detailed methodology of the PRA framework, please refer to our <a href="paper.html">forthcoming paper</a>.</p> -->
        </div>
    </main>
    <div class="header-bar">‎</div>
    <footer>
        <div class="footer-content">
            <div class="footer-title">CENTER FOR AI RISK MANAGEMENT & ALIGNMENT</div>
            <div class="footer-copyright">COPYRIGHT © 2024 CENTER FOR AI RISK MANAGEMENT & ALIGNMENT - ALL RIGHTS RESERVED.</div>
        </div>
    </footer>
</body>
</html>