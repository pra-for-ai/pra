<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex">
    <title>PRA Framework: Probabilistic Risk Assessment for AI</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div class="header-content">
            <div class="header-title">
                <h1><a href="index.html" style="color: #F5F5F5; text-decoration: none;">PRA Framework</a></h1>
                <div class="subtitle">Probabilistic Risk Assessment for AI</div>
            </div>
            <nav>
                <a href="index.html">OVERVIEW</a>
                <a href="need.html">NEED</a>
                <a href="taxonomy.html">TAXONOMY</a>
                <a href="paper.html">PAPER</a>
                <a href="workbook.html">WORKBOOK</a>
            </nav>
        </div>
    </header>
    <div class="header-bar">‎</div>
    <main>
        <div class="content">
            <h2 style="width: 100%; font-weight: normal; color: #333333; margin-top: 0cm;">Introduction to the PRA Framework</h2>
            <p>The rapid advancement of artificial intelligence is ushering in increasingly capable and versatile systems. As AI technologies evolve to handle a broader range of tasks and environments, their increasingly complex and indirect pathways of influence introduce a wide array of potential harms. This complexity, combined with the profusion of possibilities, often escapes traditional methods of evaluation, making a more accommodative risk assessment crucial for covering the entire threat surface. </p>
            <figure>
                <img src="Images/useflow.png" alt="PRA Framework Risk Assessment Flowchart" class="flowchart">
                <figcaption>Figure 1: An overview of the Probabilistic Risk Assessment (PRA) framework.</figcaption>
            </figure>
            <p>The Probabilistic Risk Assessment (PRA) framework for AI addresses this need by offering an approach to address the specific challenges posed by general-purpose AI systems. This framework is detailed in a forthcoming research paper and accompanying workbook, providing both conceptual foundations and practical guidance for conducting an assessment.
            </p>
            <h3>Purpose and Scope</h3>
            <p>As AI systems become more complex and influential, a portfolio of systematic approaches are needed to assess and manage their risks. The PRA framework aims to contribute lateral lessons from other domain areas to that portfolio, providing a systematic approach to analyzing the manifest and potential sources of harm associated with general-purpose AI systems. Its primary goals are to:</p>
            
            <ol>
                <li>Offer a structured approach for analysis of both direct hazards and their complex interactions that could lead to harm. </li>
                <li>Facilitate generation of risk likelihood levels estimates across a broad spectrum of potential hazards, with accommodation for potential risk pathways and second-order effects.</li>
                <li>Enhance societal safety by guiding the identification and assessment of risks posed by cutting-edge AI systems, by being a leading indicator rather than simply a retrospective one.</li>
            </ol>

            <h3>Prospective Analysis</h3>
            <p>The PRA framework emphasizes prospective analysis as a critical component in assessing AI risks. This forward-looking approach is essential due to the rapid advancement and evolving nature of AI technologies. Prospective analysis allows for:</p>
            <ul>
                <li>Anticipation of potential hazards before they manifest</li>
                <li>Modeling of complex, indirect, and potentially unforeseen consequences</li>
                <li>Early identification of critical control points and safety measures</li>
            </ul>

            <h3>Assessment Process</h3>
            <ol>
                <li>Select Complexity of Assessment</li>
                <li>Execute Assessment:
                    <ol type="a">
                        <li>Choose Aspects</li>
                        <li>Generate Risk Scenarios:
                            <ol type="i">
                                <li>Fault Tree Analysis</li>
                                <li>Utilizing Plausible Qualifiers and Risk Detail Table</li>
                                <li>Expert Elicitation Methods</li>
                                <li>Capability and Domain Knowledge Levels Tables</li>
                                <li>Advanced Qualitative Risk Identification Techniques</li>
                                <li>Scenario Development and Analysis</li>
                            </ol>
                        </li>
                        <li>Analyze Harm Severity Levels</li>
                        <li>Determine Likelihood Levels</li>
                        <li>Risk Levels Estimates Generated</li>
                    </ol>
                </li>
                <li>Review Report Card</li>
            </ol>

            <figure>
                <img src="Images/assessmentflow.jpg" alt="PRA Framework Assessment Process Flowchart" class="flowchart">
                <figcaption>Figure 2: An overview of the risk assessment process flow in PRA.</figcaption>
            </figure>

            <p>For detailed instructions on conducting assessments and a deeper understanding of the conceptual foundations, readers are encouraged to consult the forthcoming research paper and the accompanying workbook, which includes the User Guide (v0.9.0-alpha)).</p>
            
            <h3>Foundational Background</h3>
            <p>Traditional probabilistic risk assessment (PRA) often begins by assuming an initiating event. In manufacturing, this event might be the failure of a specific piece of equipment or component. In AI, this event might be "jailbreaking" and subsequent use of the AI by a hostile actor, fine-tuning that changes its behavior, a mistake or hallucination of important information, or a sudden jump in capabilities.</p>

            <h3>Impact Types:</h3>
            <ul>
                <li><strong>Direct results</strong> of the failure or event - e.g. a manufacturing shutdown; a deepfake campaign; an AI becoming harder to control, understand, or steer.</li>
                <li><strong>Indirect results</strong> from a series of related events - e.g. increased wear and tear on nearby equipment; erosion of trust; structurally deleterious job loss; intentional subversion and manipulation by AI systems or by hostile actors prompting them.</li>
                <li><strong>Cascading results</strong> involving secondary or simultaneous failures - e.g. a breaker fails to close, turning a short circuit into a much larger grid shutdown; a safety system fails, causing injury; an AI system gains multiple dangerous capabilities at once; an AI system gains the capacity to improve itself or design successor systems without human intervention.</li>
            </ul>

            <p>Understanding and modeling how these causal pathways may lead to harmful results - and estimating the likelihood of each intermediate possibility - is an important part of risk assessment. In AI, these pathways are particularly complex due to the system’s evolving and often unpredictable nature. By adopting traditional PRA methods to the specific case poised by AI, we can provide a more structured and systematic approach to assessing these emerging threats.</p>
            
            <figure>
                <img src="Images/tree_flow.png" alt="An event tree of a jailbreak attack." class="flowchart">
                <figcaption>Figure 3: An event tree of a jailbreak attack.</figcaption>
            </figure>

            <p>The PRA framework asks assessors to think through potential failure pathways when generating risk scenarios. This approach incorporates elements of fault tree analysis, where one works backwards from an undesired event to identify its causes. Assessors should consider how different system components or events might interact to produce harmful outcomes. This mental process, combined with the forward-looking event tree analysis, helps capture complex risk factors and their relationships. Together, these techniques lead to more comprehensive scenario generation, addressing the unique challenges posed by AI systems.</p>
            
            <h2>Current State of the Framework</h2>
            <div class="framework-status">
                <p><strong>Workbook (v0.9.0-alpha):</strong> An alpha version with functional components, available in early Q4 2024. <a href="workbook.html">Learn more.</a></p>
                <p><strong>Research Paper:</strong> Scheduled for release in Q4 2024. <a href="paper.html">Learn more.</a></p>
            </div>
            <h2>Contact Us</h2>  
            <p>The PRA framework is continuously evolving to address the rapidly advancing field of AI risk assessment. We value your insights and feedback to help refine our framework and ensure it remains as useful as it can be in AI safety practice. Whether you have questions, suggestions, or want to discuss customizing the framework for your specific needs, we encourage you to reach out to us via email:</p>
            <div class="contact-info">
                <p>
                    PRA<img src="Images/at-sign-symbol.png" alt="--" class="at-symbol">carma<img src="Images/fullstop.png" alt="-" class="fs-symbol">org
                </p>
            </div>
            <p>We appreciate your interest in the PRA for AI framework.</p>
            <h2>About CARMA</h2>
            <p>The <a href="https://carma.org/" style="color: #007bff; text-decoration: none;" onmouseover="this.style.textDecoration='underline'" onmouseout="this.style.textDecoration='none'" target="_blank">Center for AI Risk Management & Alignment</a> (CARMA) is a project of Social & Environmental Entrepreneurs, Inc., a 501(c)(3) nonprofit public charity, and is dedicated to managing risks associated with transformative artificial intelligence. Our mission is to lower the risks to humanity and the biosphere from transformative AI. CARMA's research addresses critical challenges in AI risk assessment and governance. We focus on methods for mapping and sizing of outsized risks from transformative AI, and conduct research on technical AI governance and public security measures.</p>
        </div>
    </main>
    <div class="header-bar">‎</div>
    <footer>
        <div class="footer-content">
            <div class="footer-title">CENTER FOR AI RISK MANAGEMENT & ALIGNMENT</div>
            <div class="footer-copyright">COPYRIGHT © 2024 CENTER FOR AI RISK MANAGEMENT & ALIGNMENT - ALL RIGHTS RESERVED.</div>
        </div>
    </footer>

    <div id="imageModal" class="modal">
        <img class="modal-content" id="modalImage">
    </div>
    <script>
        if (window.matchMedia("(max-width: 768px)").matches) {

            var modal = document.getElementById('imageModal');
            var modalImg = document.getElementById("modalImage");
        
            var images = document.querySelectorAll('img.flowchart');
        
            images.forEach(function(img) {
                img.style.cursor = 'pointer'; 
                img.addEventListener('click', function() {
                    modal.style.display = "block";
                    modalImg.src = this.src;
                });
            });
        
        
            modal.onclick = function(event) {
                if (event.target == modal) {
                    modal.style.display = "none";
                }
            }
        }
        </script>
        
</body>
</html>