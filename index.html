<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex">
    <title>PRA Framework: Probabilistic Risk Assessment for AI</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div class="header-content">
            <div class="header-title">
                <h1><a href="index.html" style="color: #F5F5F5; text-decoration: none;">PRA Framework</a></h1>
                <div class="subtitle">Probabilistic Risk Assessment for AI</div>
            </div>
            <nav>
                <a href="index.html">INTRODUCTION</a>
                <a href="paper.html">PAPER</a>
                <a href="workbook.html">WORKBOOK</a>
                <a href="contact.html">CONTACT</a>
            </nav>
        </div>
    </header>
    <div class="header-bar">‎</div>
    <main>
        <div class="content">
            <h2 style="width: 100%; font-weight: normal; color: #333333; margin-top: 0cm;">Introduction to the PRA Framework</h2>
            <p>The rapid advancement of artificial intelligence is ushering in increasingly capable and versatile systems. As AI technologies evolve to handle a broader range of tasks and environments, and increasing indirection in their direction, they introduce a complex array of potential risks, a profusion of possibilities that are sometimes tricky to witness and evaluate using traditional methods. This makes a more accommodative risk assessment crucial for covering the threat surface.</p>
            <figure>
                <img src="Images/useflow.png" alt="PRA Framework Risk Assessment Flowchart" class="flowchart">
                <figcaption>Figure 1: An overview of the Probabilistic Risk Assessment (PRA) framework.</figcaption>
            </figure>
            <p>The Probabilistic Risk Assessment (PRA) framework for AI addresses this need by offering an approach to address the specific challenges posed by general-purpose AI systems. This framework is detailed in a forthcoming research paper and accompanying workbook, providing both conceptual foundations and practical guidance for conducting an assessment.</p>
            <h3>Purpose and Scope</h3>
            <p>As AI systems become more complex and influential, a portfolio of systematic approaches are needed to assess and manage their risks. The PRA framework aims to contribute lateral lessons from other domain areas to that portfolio, providing a systematic approach to analyzing the manifest and prospective risks associated with general-purpose AI systems. Its primary goals are to:</p>
            <ol>
                <li>Offer a structured approach for analysis of both direct hazards and complex risk interactions.</li>
                <li>Facilitate generation of risk likelihood levels estimates across a broad spectrum of potential hazards, with accommodation for potential risk pathways and second-order effects.</li>
                <li>Enhance societal safety by guiding the identification and assessment of risks posed by cutting-edge AI systems, by being a leading indicator rather than simply a retrospective one.</li>
            </ol>
            <p>This framework focuses on risk assessment and estimation, providing data and insights to support decision-makers in their evaluation of AI system safety and acceptability.</p>

            <h3>Applicability</h3>
            <p>The PRA framework provides a versatile approach to assessing risks from advanced and general-purpose AI systems across a wide spectrum of architectures and capabilities. This framework is also designed to facilitate the analysis of a broad range of threats to the world that can be attributable to an advanced AI technology, supporting both current and future systems.</p>

            <h2>Assessment Process</h2>
            <p>The PRA framework follows a structured process to assess AI risks:</p>
            <ol>
                <li>Select Complexity of Assessment</li>
                <li>Execute Assessment:
                    <ul>
                        <li>Choose Aspects</li>
                        <li>Generate Risk Scenarios:
                            <ol>
                                <li>De novo</li>
                                <li>Utilizing Plausible Qualifiers and Risk Detail Table</li>
                                <li>Expert Elicitation Methods</li>
                                <li>Capability and Domain Knowledge Levels Tables</li>
                                <li>Advanced Qualitative Risk Identification Techniques</li>
                                <li>Scenario Development and Analysis</li>
                            </ol>
                        </li>
                        <li>Analyze Harm Severity Levels</li>
                        <li>Determine Likelihood Levels</li>
                        <li>Risk Levels Estimates Generated</li>
                    </ul>
                </li>
                <li>Review Report Card</li>
            </ol>

            <figure>
                <img src="Images/assessmentflow.jpg" alt="PRA Framework Assessment Process Flowchart" class="flowchart">
                <figcaption>Figure 2: An overview of the risk assessment process flow in PRA.</figcaption>
            </figure>

            <p>For detailed instructions on conducting assessments and a deeper understanding of the conceptual foundations, readers are encouraged to consult the forthcoming research paper and the accompanying workbook, which includes the User Guide (beta 0.9).</p>
            
            <h2>Foundational Background</h2>
            <p>Traditional probabilistic risk assessment (PRA) often begins by assuming an initiating event. In manufacturing, this event might be the failure of a specific piece of equipment or component. In AI, this event might be “jailbreaking” and subsequent use of the AI by a hostile actor, fine-tuning that changes its behavior, a mistake or hallucination of important information, or a sudden jump in capabilities. </p>
            
            <p>Impacts caused by such an event can be of several types:</p>

            <ul>
                <li><strong>Direct results of the failure or event</strong> - e.g. a manufacturing shutdown; a deepfake campaign; an AI becoming harder to control, understand, or steer.</li>
                <li><strong>Indirect results from a series of related events</strong> - e.g. increased wear and tear on nearby equipment; erosion of trust; structurally deleterious job loss; intentional subversion and manipulation by AI systems or by hostile actors prompting them.</li>
                <li><strong>Cascading results involving secondary or simultaneous failures</strong> - e.g. a breaker fails to close, turning a short circuit into a much larger grid shutdown; a safety system fails, causing injury; an AI system gains multiple dangerous capabilities at once; an AI system gains the capacity to improve itself or design successor systems without human intervention.</li>
            </ul>
            <p>Understanding and modeling how these causal pathways may lead to harmful results - and estimating the likelihood of each intermediate possibility - is an important part of risk assessment. In AI, these pathways are particularly complex due to the system’s evolving and often unpredictable nature. By adopting traditional PRA methods to the specific case poised by AI, we can provide a more structured and systematic approach to assessing these emerging threats.</p>
            <h2>Current State of the Framework</h2>
            <div class="framework-status">
                <p><strong>Research Paper:</strong> Scheduled for release in early Q4 2024. <a href="paper.html">Learn more.</a></p>
                <p><strong>Workbook (v0.9):</strong> A forthcoming working beta with functional components, scheduled for release in early Q4 2024. <a href="workbook.html">Learn more.</a></p>
            </div>
            <h2>Authors</h2>
            The Comprehensive Risk Assessment Team at the Center for AI Risk Management & Alignment (CARMA). CARMA is dedicated to managing risks associated with transformative AI, focusing on methods for accurate mapping and sizing of outsized risks from AI systems.
            
        </div>
    </main>
    <div class="header-bar">‎</div>
    <footer>
        <div class="footer-content">
            <div class="footer-title">CENTER FOR AI RISK MANAGEMENT & ALIGNMENT</div>
            <div class="footer-copyright">COPYRIGHT © 2024 CENTER FOR AI RISK MANAGEMENT & ALIGNMENT - ALL RIGHTS RESERVED.</div>
        </div>
    </footer>
</body>
</html>